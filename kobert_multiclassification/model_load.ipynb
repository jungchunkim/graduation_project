{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset \n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from pathlib import Path\n",
    "from utils import BERTClassifier\n",
    "\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 7,\n",
    "                 dr_rate = None,\n",
    "                 params = None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids = torch.zeros_like(segment_ids).long(), attention_mask=attention_mask.float().to(token_ids.device))\n",
    "\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "\n",
    "        return self.classifier(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 측정할 사용자 이름 적으면 된다.(사용자로부터 받아와야한다.)\n",
    "user_name=\"신상윤\"\n",
    "\n",
    "#대화 리스트 들어갈 곳\n",
    "all_conversation_arr=[]\n",
    "\n",
    "# 요일 없애야하는것\n",
    "remove_characters=\"년월일-월화수목금토요 \" \n",
    "cur_time=\"\"\n",
    "conversation=\"\"\n",
    "\n",
    "#여기다가 원하는 카카오 데이터 셋(txt) 집어 넣으면 됨 ( 여기서는 KakaoTalk_20210426_2042_40_926_김준홍.txt 를 넣었음)\n",
    "f = open(\"KakaoTalk_20210503_1909_44_475_신상윤.txt\", 'r',encoding='UTF8')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line:  #마지막에 도달했을 때 반복문 빠져나옴\n",
    "        break\n",
    "    #요일이 시작되는 경우\n",
    "    if line[:5]==\"-----\": \n",
    "        line=''.join(x for x in line if x not in remove_characters)\n",
    "        cur_time=line #  년, 월, 일 형태로 받는다.(ex 2020120)\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            # line 이 빈 값일 때 or 끝났을 때\n",
    "            if line[:5]==\"-----\" or not line:\n",
    "                break\n",
    "            #사용자의 이름인 것만 받아온다. (한줄이 50정도max -> 한줄만 받자)\n",
    "            if line[1:len(user_name)+1]==user_name:\n",
    "                conversation=line[16:]\n",
    "                all_conversation_arr.append([user_name,cur_time,conversation])\n",
    "                \n",
    "\n",
    "for i in range(len(all_conversation_arr)):\n",
    "    all_conversation_arr[i][1]=re.sub(\"\\n\",\"\",all_conversation_arr[i][1])\n",
    "    all_conversation_arr[i][2]=re.sub(\"\\n\",\"\",all_conversation_arr[i][2])\n",
    "\n",
    "# 이상한 문자 있는 문장  지워버림 - 훨씬 깔끔하게 나옴\n",
    "remove_letters=\"0123456789ㅂㅈㄷㄱㅅㅕㅑㅐㅔ[ㅁㄴㅇㅃㅉㄸㄲㅆㄹㅎ,_ㅗㅓㅏ※ㅣ;]'ㅋㅌㅊ)=(ㅠㅜㅍㅡabcdefghijklmnopqrstuvwxyz/QWERTYUIOPASDFGHJKLZXCVBNM#%-\\\":\"\n",
    "for i in reversed(range(len(all_conversation_arr))):\n",
    "    for x in all_conversation_arr[i][2]:\n",
    "        if x in remove_letters:\n",
    "            del all_conversation_arr[i]\n",
    "            break\n",
    "\n",
    "# 길이가 2 이하인 문자열 제거\n",
    "for i in reversed(range(len(all_conversation_arr))):\n",
    "    if len(all_conversation_arr[i][2])<=10 or len(all_conversation_arr[i][2]) > 56:\n",
    "        del all_conversation_arr[i]\n",
    "        \n",
    "all_conversation_arr.reverse()\n",
    "# 최근 대화 50개 리스트\n",
    "num_50_conversation=all_conversation_arr[:50]\n",
    "# 최근 대화 100개 리스트\n",
    "num_100_conversation=all_conversation_arr[:100]\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # _model_dir = \"/home/k4ke/kobert/saves\"\n",
    "    # model_dir = Path(_model_dir)\n",
    "    # model_config = Config(json_path = model_dir / 'config.json')\n",
    "\n",
    "    # Vocab and Tokenizer\n",
    "    tokenizer = get_tokenizer()\n",
    "    \n",
    "    bertmodel, vocab = get_pytorch_kobert_model()\n",
    "    # token_to_idx = vocab.token_to_idx\n",
    "    #\n",
    "    # # vocab_size = len(token_to_idx)\n",
    "    # print(\"len(toekn_to_idx): \", len(token_to_idx))\n",
    "    #\n",
    "    # with open(model_dir / \"token2idx_vocab.json\", 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(token_to_idx, f, ensure_ascii=False, indent=4)\n",
    "    #\n",
    "    # # save vocab & tokenizer\n",
    "    # with open(model_dir / \"vocab.pkl\", 'wb') as f:\n",
    "    #     pickle.dump(vocab, f)\n",
    "    #\n",
    "    # # load vocab & tokenizer\n",
    "    # with open(model_dir / \"vocab.pkl\", 'rb') as f:\n",
    "    #     vocab = pickle.load(f)\n",
    "\n",
    "    # tokenizer = Tokenizer(vocab=vocab, split_fn=ptr_tokenizer, pad_fn=keras_pad_fn, maxlen=64)\n",
    "    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "    model = BERTClassifier(bertmodel)\n",
    "\n",
    "    # load model\n",
    "    model_dict = model.state_dict()\n",
    "    # checkpoint = torch.load(\"./experiments/base_model_with_crf_val/best-epoch-12-step-1000-acc-0.960.bin\", map_location=torch.device('cpu'))\n",
    "    # checkpoint = torch.load(\"/home/k4ke/kobert/saves/best-epoch-5-f1-0.916.bin\", map_location = torch.device('cpu'))\n",
    "    checkpoint = torch.load(\"C:/Users\\KIMJOONHONG/ml/datasets/saves/best-epoch-36-f1-0.732.bin\", map_location=torch.device('cpu'))\n",
    "    convert_keys = {}\n",
    "    for k, v in checkpoint['model_state_dict'].items():\n",
    "        new_key_name = k.replace(\"module.\", '')\n",
    "        if new_key_name not in model_dict:\n",
    "            print(\"{} is not int model_dict\".format(new_key_name))\n",
    "            continue\n",
    "        convert_keys[new_key_name] = v\n",
    "\n",
    "    model.load_state_dict(convert_keys)\n",
    "    # model.load_state_dict(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    emo_dict = {0: '공포', 1: '놀람', 2: '분노', 3: '슬픔', 4: '중립', 5: '행복', 6: '혐오'}\n",
    "    emo_dict2 = {0: '공포', 1: '분노', 2: '슬픔', 3: '행복', 4: '혐오'}\n",
    "    result=np.zeros((1,7),dtype=float)\n",
    "    #print(arr)\n",
    "    for i in all_conversation_arr:\n",
    "    #while True:\n",
    "        #_sentence = input(\"input: \")\n",
    "        _sentence=str(i[2])\n",
    "        if _sentence == '-1':\n",
    "            break\n",
    "        transform = nlp.data.BERTSentenceTransform(tok, max_seq_length=64, pad=True, pair=False)\n",
    "        # self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        sentence = [transform([_sentence])]\n",
    "        #data_train = BERTDataset(sentence, 0, 1, tok, 64, True, False)\n",
    "        dataloader = torch.utils.data.DataLoader(sentence, batch_size=1)\n",
    "        _token_ids = dataloader._index_sampler.sampler.data_source\n",
    "        # print(_token_ids)\n",
    "        # print(_token_ids[0])\n",
    "        # print(_token_ids[0][0])\n",
    "        _t = torch.from_numpy(_token_ids[0][0])\n",
    "        _t = _t.tolist()\n",
    "        token_ids = torch.tensor(_t, dtype=torch.long).unsqueeze(0).cuda()\n",
    "        val_len = torch.tensor([len(token_ids[0])], dtype=torch.long).cuda()\n",
    "        # val_len = torch.tensor([len(token_ids)], dtype=torch.long).cuda()\n",
    "\n",
    "        _s = torch.from_numpy(_token_ids[0][1])\n",
    "        _s = _s.tolist()\n",
    "        segment_ids = torch.tensor(_s, dtype=torch.long).unsqueeze(0).cuda()\n",
    "        # segment_ids = torch.from_numpy(_token_ids[0][1]).unsqueeze(0)\n",
    "        # segment_ids = torch.zeros()\n",
    "        # print(len(token_ids)) # 1\n",
    "\n",
    "        out = model(token_ids, val_len, segment_ids)\n",
    "        out_idx = np.argmax(out.cpu().detach().numpy())\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        score = softmax(out).cpu().detach().numpy()\n",
    "        result+=score\n",
    "        #print(\"out: \", out)\n",
    "        print(\"input:\",_sentence)\n",
    "        print(out_idx, emo_dict[out_idx])\n",
    "        #print(\"score: \", score)\n",
    "    print(emo_dict2[np.delete(result,[1,4]).argmax()])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
