{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIMJOONHONG\\Anaconda3\\envs\\JH\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import json\n",
    "# import pickle\n",
    "# import logging\n",
    "import os, sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "from torch.utils.data import Dataset #, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from pathlib import Path\n",
    "from utils import BERTClassifier\n",
    "\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda:0\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n",
      "input: 나 지금 너무 눈물나 ㅠㅠ\n",
      "out:  tensor([[ 1.2232, -0.0669, -1.8283,  4.9234,  1.5774, -2.1032, -2.7579]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "3 슬픔\n",
      "score:  [[2.3118014e-02 6.3633285e-03 1.0931710e-03 9.3521851e-01 3.2945052e-02\n",
      "  8.3044672e-04 4.3149601e-04]]\n",
      "input: 아 뭐 어쩌라고 나한테!!\n",
      "out:  tensor([[-2.3251,  3.0880,  3.4174, -2.0633,  2.1222, -2.1368, -2.2351]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "2 분노\n",
      "score:  [[0.001597   0.35824665 0.49802276 0.0020749  0.13638349 0.00192793\n",
      "  0.00174735]]\n",
      "input: 자기야 사랑해 ><\n",
      "out:  tensor([[-1.1516,  6.9373, -1.9279, -1.8856,  1.8791, -2.5480, -2.0795]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "1 놀람\n",
      "score:  [[3.04731919e-04 9.92901206e-01 1.40217657e-04 1.46273203e-04\n",
      "  6.31167274e-03 7.54214780e-05 1.20486395e-04]]\n",
      "input: 특이하네 이건 소금이 안들어가네\n",
      "out:  tensor([[-1.2446,  7.0159, -1.8439, -1.8639,  1.6631, -2.4347, -2.0818]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "1 놀람\n",
      "score:  [[2.5711430e-04 9.9456501e-01 1.4121321e-04 1.3840546e-04 4.7087548e-03\n",
      "  7.8212819e-05 1.1130741e-04]]\n",
      "input: -1\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # _model_dir = \"/home/k4ke/kobert/saves\"\n",
    "    # model_dir = Path(_model_dir)\n",
    "    # model_config = Config(json_path = model_dir / 'config.json')\n",
    "\n",
    "    # Vocab and Tokenizer\n",
    "    tokenizer = get_tokenizer()\n",
    "    \n",
    "    bertmodel, vocab = get_pytorch_kobert_model()\n",
    "    # token_to_idx = vocab.token_to_idx\n",
    "    #\n",
    "    # # vocab_size = len(token_to_idx)\n",
    "    # print(\"len(toekn_to_idx): \", len(token_to_idx))\n",
    "    #\n",
    "    # with open(model_dir / \"token2idx_vocab.json\", 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(token_to_idx, f, ensure_ascii=False, indent=4)\n",
    "    #\n",
    "    # # save vocab & tokenizer\n",
    "    # with open(model_dir / \"vocab.pkl\", 'wb') as f:\n",
    "    #     pickle.dump(vocab, f)\n",
    "    #\n",
    "    # # load vocab & tokenizer\n",
    "    # with open(model_dir / \"vocab.pkl\", 'rb') as f:\n",
    "    #     vocab = pickle.load(f)\n",
    "\n",
    "    # tokenizer = Tokenizer(vocab=vocab, split_fn=ptr_tokenizer, pad_fn=keras_pad_fn, maxlen=64)\n",
    "    tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "    model = BERTClassifier(bertmodel)\n",
    "\n",
    "    # load model\n",
    "    model_dict = model.state_dict()\n",
    "    # checkpoint = torch.load(\"./experiments/base_model_with_crf_val/best-epoch-12-step-1000-acc-0.960.bin\", map_location=torch.device('cpu'))\n",
    "    # checkpoint = torch.load(\"/home/k4ke/kobert/saves/best-epoch-5-f1-0.916.bin\", map_location = torch.device('cpu'))\n",
    "    checkpoint = torch.load(\"C:/Users\\KIMJOONHONG/ml/datasets/saves/best-epoch-13-f1-0.715.bin\", map_location=torch.device('cpu'))\n",
    "    convert_keys = {}\n",
    "    for k, v in checkpoint['model_state_dict'].items():\n",
    "        new_key_name = k.replace(\"module.\", '')\n",
    "        if new_key_name not in model_dict:\n",
    "            print(\"{} is not int model_dict\".format(new_key_name))\n",
    "            continue\n",
    "        convert_keys[new_key_name] = v\n",
    "\n",
    "    model.load_state_dict(convert_keys)\n",
    "    # model.load_state_dict(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    #model.resize_token_embeddings(len(tokenizer))\n",
    "    emo_dict = {0: '공포', 1: '놀람', 2: '분노', 3: '슬픔', 4: '중립', 5: '행복', 6: '혐오'}\n",
    "\n",
    "\n",
    "    while(True):\n",
    "        _sentence = input(\"input: \")\n",
    "        if _sentence == '-1':\n",
    "            break\n",
    "        transform = nlp.data.BERTSentenceTransform(tok, max_seq_length=64, pad=True, pair=False)\n",
    "        # self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        sentence = [transform([_sentence])]\n",
    "        #data_train = BERTDataset(sentence, 0, 1, tok, 64, True, False)\n",
    "        dataloader = torch.utils.data.DataLoader(sentence, batch_size=1)\n",
    "        _token_ids = dataloader._index_sampler.sampler.data_source\n",
    "        # print(_token_ids)\n",
    "        # print(_token_ids[0])\n",
    "        # print(_token_ids[0][0])\n",
    "        _t = torch.from_numpy(_token_ids[0][0])\n",
    "        _t = _t.tolist()\n",
    "        token_ids = torch.tensor(_t, dtype=torch.long).unsqueeze(0).cuda()\n",
    "        val_len = torch.tensor([len(token_ids[0])], dtype=torch.long).cuda()\n",
    "        # val_len = torch.tensor([len(token_ids)], dtype=torch.long).cuda()\n",
    "\n",
    "        _s = torch.from_numpy(_token_ids[0][1])\n",
    "        _s = _s.tolist()\n",
    "        segment_ids = torch.tensor(_s, dtype=torch.long).unsqueeze(0).cuda()\n",
    "        # segment_ids = torch.from_numpy(_token_ids[0][1]).unsqueeze(0)\n",
    "        # segment_ids = torch.zeros()\n",
    "        # print(len(token_ids)) # 1\n",
    "\n",
    "        out = model(token_ids, val_len, segment_ids)\n",
    "        out_idx = np.argmax(out.cpu().detach().numpy())\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        score = softmax(out).cpu().detach().numpy()\n",
    "\n",
    "        print(\"out: \", out)\n",
    "        print(out_idx, emo_dict[out_idx])\n",
    "        print(\"score: \", score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
