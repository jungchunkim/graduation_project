{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "import os, sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "# setproctitle.setproctitle('[k4ke] emoji_v0')\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import CheckpointManager, SummaryManager\n",
    "\n",
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 7,\n",
    "                 dr_rate = None,\n",
    "                 params = None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=torch.zeros_like(segment_ids), attention_mask=attention_mask.float().to(token_ids.device))\n",
    "\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        else:\n",
    "            out = pooler\n",
    "\n",
    "        return self.classifier(out)\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        # sent_idx: 0 (데이터)\n",
    "        # label_idx : 1 (레이블)\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "## Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 32 # 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 60\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5\n",
    "\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# gluonnlp.data.TSVDataset\n",
    "# field_indices (list of int or None, default None)\n",
    "# If set, for each sample, only fields with provided indices are selected as the output. Otherwise all fields are returned.\n",
    "\n",
    "# num_discard_samples (int, default 0)\n",
    "# Number of samples discarded at the head of the first file.\n",
    "dataset = nlp.data.TSVDataset(\"all_korea_final.txt\", field_indices=[1, 2], num_discard_samples = 1,encoding='cp949')\n",
    "\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "# trainset & testset\n",
    "for j, d in enumerate(dataset):\n",
    "    i = np.random.randint(2)\n",
    "    if i == 0:\n",
    "        dataset_train.append(d)\n",
    "    elif i == 1 and len(dataset_test) <= 10000:\n",
    "        dataset_test.append(d)\n",
    "    else:\n",
    "        dataset_train.append(d)\n",
    "\n",
    "_count = [int(s[1]) for s in dataset_test[:]] # label\n",
    "n_appear  = [_count.count(i) for i in range(7)] # total label\n",
    "\n",
    "\n",
    "# dataset_train = nlp.data.TSVDataset(\"sentiment2.tsv\", field_indices=[1, 2], num_discard_samples = 1)\n",
    "# dataset_test = nlp.data.TSVDataset(\"sentiment2.tsv\", field_indices=[1, 2], num_discard_samples = 1)\n",
    "\n",
    "_count = [int(s[1]) for s in dataset_train[:]] # label\n",
    "n_appear  = [_count.count(i) for i in range(7)] # total label\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "# num_workers: multi-process data loading\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 0, shuffle = True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 0, shuffle = True)\n",
    "\n",
    "model = BERTClassifier(bertmodel, dr_rate = 0.5).to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "def weighted_corss_entropy_loss(n_appear: list):\n",
    "    weights = [sum(n_appear) / n for n in n_appear]\n",
    "    loss_fn = nn.CrossEntropyLoss(weight = torch.FloatTensor(weights).cuda())\n",
    "    return loss_fn\n",
    "\n",
    "loss_fn = weighted_corss_entropy_loss(n_appear)\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# torch.max(X, 1)\n",
    "\n",
    "# tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
    "#         [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
    "#         [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
    "#         [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
    "# >>> torch.max(a, 1)\n",
    "# torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n",
    "def calc_accuracy(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "def calc_F1(X, Y):\n",
    "    \"\"\"\n",
    "    @author: k4ke\n",
    "    \"\"\"\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    # train_f1 = f1_score(Y.cpu().numpy(), max_indices.cpu().numpy(), average = 'weighted')\n",
    "    train_f1 = f1_score(Y.cpu().numpy(), max_indices.cpu().numpy(), average = 'micro')\n",
    "    return train_f1\n",
    "\n",
    "# save\n",
    "# model_dir: Directory containing config.json of model\n",
    "model_dir = \"./saves\"\n",
    "tb_writer = SummaryWriter('{}/runs'.format(model_dir))\n",
    "checkpoint_manager = CheckpointManager(model_dir)\n",
    "summary_manager = SummaryManager(model_dir)\n",
    "\n",
    "# train\n",
    "best_dev_f1 = -sys.maxsize # min\n",
    "\n",
    "# early stop\n",
    "# score = []\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    train_f1 = 0.0\n",
    "    test_f1 = 0.0\n",
    "    _loss = 0.0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        train_f1 += calc_F1(out, label)\n",
    "\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {} train F1 {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1) , train_f1 / (batch_id+1)))\n",
    "\n",
    "    print(\"epoch {} train acc {} train F1 {}\".format(e+1, train_acc / (batch_id+1), train_f1 / (batch_id+1)))\n",
    "    tr_summary = {'acc': train_acc / (batch_id + 1), 'f1': train_f1 / (batch_id+1)}\n",
    "\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "        test_f1 += calc_F1(out, label)\n",
    "    print(\"epoch {} test acc {} test_f1 {}\".format(e+1, test_acc / (batch_id+1), test_f1 / (batch_id+1)))\n",
    "    eval_summary = {'acc': test_acc / (batch_id+1), 'f1': test_f1 / (batch_id+1)}\n",
    "\n",
    "    # save model\n",
    "    output_dir = \"./output/checkpoints/epoch-{}\".format(e + 1)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    print(\"model checkpoint: \", output_dir)\n",
    "\n",
    "    state = {'global_step': e + 1,\n",
    "             'model_state_dict': model.state_dict(),\n",
    "             'opt_state_dict': optimizer.state_dict()}\n",
    "\n",
    "    summary = {'train': tr_summary, 'eval': eval_summary}\n",
    "    summary_manager.update(summary)\n",
    "    print(\"summary: \", summary)\n",
    "    summary_manager.save('summary.json')\n",
    "\n",
    "    # save\n",
    "    is_best = eval_summary['f1'] >= best_dev_f1\n",
    "\n",
    "    if is_best:\n",
    "        best_dev_f1 = eval_summary['f1']\n",
    "        checkpoint_manager.save_checkpoint(state, 'best-epoch-{}-f1-{:.3f}.bin'.format(e + 1, best_dev_f1))\n",
    "        print(\"model checkpoint has been saved: best-epoch-{}-f1-{:.3f}.bin\".format(e + 1, best_dev_f1))\n",
    "\n",
    "        ## print classification report and save confusion matrix\n",
    "        # cr_save_path = model_dir / 'best-epoch-{}-f1-{:.3f}-cr.csv'.format(e + 1, best_dev_f1)\n",
    "        # cm_save_path = model_dir / 'best-epoch-{}-f1-{:.3f}-cm.png'.format(e + 1, best_dev_f1)\n",
    "    else:\n",
    "        torch.save(state, os.path.join(output_dir, 'model-epoch-{}-f1-{:.3f}.bin'.format(e + 1, eval_summary[\"f1\"])))\n",
    "        print(\"model checkpoint has been saved: best-epoch-{}-f1-{:.3f}.bin\".format(e + 1, eval_summary['f1']))\n",
    "\n",
    "    # score.append(eval_summary['f1'])\n",
    "    #\n",
    "    # # early stop\n",
    "    # if np.std(score[-min(5, len(score)):]) < 0.1:\n",
    "    #     sys.exit()\n",
    "    # else:\n",
    "    #     print(\"not yet\")\n",
    "tb_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
